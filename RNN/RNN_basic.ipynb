{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 문자 -> 인덱스 변환 함수\n",
    "def char_to_index(char, char_to_idx):\n",
    "    return char_to_idx[char]\n",
    "\n",
    "# 문자열 -> 원-핫 인코딩 변환 함수\n",
    "def string_to_one_hot(string, char_to_idx):\n",
    "    tensor = torch.zeros(len(string), len(char_to_idx))\n",
    "    for idx, char in enumerate(string):\n",
    "        tensor[idx][char_to_index(char, char_to_idx)] = 1\n",
    "    return tensor.to(device)\n",
    "\n",
    "# 데이터 수집\n",
    "response = requests.get(\"https://loripsum.net/api?plaintext&paragraphs=10000\")\n",
    "data = response.text\n",
    "sentences = data.split(\"\\n\")\n",
    "\n",
    "# 문자 집합 생성\n",
    "char_set = sorted(list(set(data)))\n",
    "char_to_idx = {char: idx for idx, char in enumerate(char_set)}\n",
    "\n",
    "# 패딩 작업을 수행하는 함수 (수정된 부분)\n",
    "def pad_sequences(sequences, max_length, padding_value):\n",
    "    padded_sequences = []\n",
    "    for sequence in sequences:\n",
    "        sequence = np.array(sequence) # 리스트를 Numpy 배열로 변환\n",
    "        padding_needed = max_length - sequence.shape[0]\n",
    "        if padding_needed > 0:\n",
    "            if sequence.ndim == 1:\n",
    "                sequence = np.hstack((sequence, np.full(padding_needed, padding_value)))\n",
    "            else:\n",
    "                sequence = np.vstack((sequence, np.full((padding_needed, sequence.shape[1]), padding_value)))\n",
    "        else:\n",
    "            sequence = sequence[:max_length]\n",
    "        padded_sequences.append(sequence)\n",
    "    return padded_sequences\n",
    "\n",
    "# 데이터셋 생성 (수정된 부분)\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, sentences, char_to_idx, max_length):\n",
    "        self.sentences = sentences\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.sentences[index]\n",
    "        input_data = string_to_one_hot(sentence[:-1], self.char_to_idx).cpu().numpy() # 원-핫 인코딩된 데이터를 Numpy 배열로 변환\n",
    "        target_data = [char_to_index(char, self.char_to_idx) for char in sentence[1:]]\n",
    "\n",
    "        # 패딩 작업 수행\n",
    "        input_data = pad_sequences([input_data], self.max_length, [0] * len(self.char_to_idx))[0] # 패딩 값으로 0 벡터를 사용\n",
    "        target_data = pad_sequences([target_data], self.max_length, 0)[0] # -100은 패딩 값입니다. 원하는 값으로 변경하세요.\n",
    "\n",
    "        input_data = torch.tensor(input_data, dtype=torch.float).to(device)\n",
    "        target_data = torch.tensor(target_data, dtype=torch.long).to(device)\n",
    "\n",
    "        return input_data, target_data\n",
    "\n",
    "# 문장의 최대 길이 설정\n",
    "max_length = max([len(sentence) for sentence in sentences])\n",
    "\n",
    "# 데이터셋 및 DataLoader 생성\n",
    "train_dataset = ToyDataset(sentences, char_to_idx, max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True) # 배치 크기를 원하는 값으로 변경하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Qua tu etiam inprudens utebare non numquam. Duo Reges: constructio interrete. Non igitur potestis voluptate omnia dirigentes aut tueri aut retinere virtutem. Itaque rursus eadem ratione, qua sum paulo ante usus, haerebitis. Restatis igitur vos; Ad corpus diceres pertinere-, sed ea, quae dixi, ad corpusne refers? Apud ceteros autem philosophos, qui quaesivit aliquid, tacet; Non est enim vitium in oratione solum, sed etiam in moribus. At multis malis affectus. </p>\n",
      "470\n",
      "46\n",
      "torch.Size([470, 46]) torch.Size([470])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0') tensor([38,  9, 21, 43, 25,  1, 42, 43,  1, 29, 42, 33, 25, 35,  1, 33, 36, 38,\n",
      "        40, 43, 28, 29, 36, 41,  1, 43, 42, 29, 26, 25, 40, 29,  1, 36, 37, 36,\n",
      "         1, 36, 43, 35, 39, 43, 25, 35,  4,  1, 13, 43, 37,  1, 22, 29, 31, 29,\n",
      "        41,  6,  1, 27, 37, 36, 41, 42, 40, 43, 27, 42, 33, 37,  1, 33, 36, 42,\n",
      "        29, 40, 40, 29, 42, 29,  4,  1, 19, 37, 36,  1, 33, 31, 33, 42, 43, 40,\n",
      "         1, 38, 37, 42, 29, 41, 42, 33, 41,  1, 44, 37, 34, 43, 38, 42, 25, 42,\n",
      "        29,  1, 37, 35, 36, 33, 25,  1, 28, 33, 40, 33, 31, 29, 36, 42, 29, 41,\n",
      "         1, 25, 43, 42,  1, 42, 43, 29, 40, 33,  1, 25, 43, 42,  1, 40, 29, 42,\n",
      "        33, 36, 29, 40, 29,  1, 44, 33, 40, 42, 43, 42, 29, 35,  4,  1, 16, 42,\n",
      "        25, 39, 43, 29,  1, 40, 43, 40, 41, 43, 41,  1, 29, 25, 28, 29, 35,  1,\n",
      "        40, 25, 42, 33, 37, 36, 29,  2,  1, 39, 43, 25,  1, 41, 43, 35,  1, 38,\n",
      "        25, 43, 34, 37,  1, 25, 36, 42, 29,  1, 43, 41, 43, 41,  2,  1, 32, 25,\n",
      "        29, 40, 29, 26, 33, 42, 33, 41,  4,  1, 22, 29, 41, 42, 25, 42, 33, 41,\n",
      "         1, 33, 31, 33, 42, 43, 40,  1, 44, 37, 41,  7,  1, 11, 28,  1, 27, 37,\n",
      "        40, 38, 43, 41,  1, 28, 33, 27, 29, 40, 29, 41,  1, 38, 29, 40, 42, 33,\n",
      "        36, 29, 40, 29,  3,  2,  1, 41, 29, 28,  1, 29, 25,  2,  1, 39, 43, 25,\n",
      "        29,  1, 28, 33, 45, 33,  2,  1, 25, 28,  1, 27, 37, 40, 38, 43, 41, 36,\n",
      "        29,  1, 40, 29, 30, 29, 40, 41, 10,  1, 11, 38, 43, 28,  1, 27, 29, 42,\n",
      "        29, 40, 37, 41,  1, 25, 43, 42, 29, 35,  1, 38, 32, 33, 34, 37, 41, 37,\n",
      "        38, 32, 37, 41,  2,  1, 39, 43, 33,  1, 39, 43, 25, 29, 41, 33, 44, 33,\n",
      "        42,  1, 25, 34, 33, 39, 43, 33, 28,  2,  1, 42, 25, 27, 29, 42,  7,  1,\n",
      "        19, 37, 36,  1, 29, 41, 42,  1, 29, 36, 33, 35,  1, 44, 33, 42, 33, 43,\n",
      "        35,  1, 33, 36,  1, 37, 40, 25, 42, 33, 37, 36, 29,  1, 41, 37, 34, 43,\n",
      "        35,  2,  1, 41, 29, 28,  1, 29, 42, 33, 25, 35,  1, 33, 36,  1, 35, 37,\n",
      "        40, 33, 26, 43, 41,  4,  1, 11, 42,  1, 35, 43, 34, 42, 33, 41,  1, 35,\n",
      "        25, 34, 33, 41,  1, 25, 30, 30, 29, 27, 42, 43, 41,  4,  1,  8,  5, 38,\n",
      "         9,  0], device='cuda:0')\n",
      "\n",
      "0\n",
      "46\n",
      "torch.Size([470, 46]) torch.Size([470])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0') tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(sentences[2]) # 첫번째 문장\n",
    "print(len(sentences[2])) # 첫번째 문자의 길이\n",
    "print(len(char_set)) #전체 문자의 개수\n",
    "i, t = train_dataset.__getitem__(2)\n",
    "print(i.size(), t.size())\n",
    "print(i, t) \n",
    "\n",
    "\n",
    "print(sentences[1]) # 첫번째 문장\n",
    "print(len(sentences[1])) # 첫번째 문자의 길이\n",
    "print(len(char_set)) #전체 문자의 개수\n",
    "i, t = train_dataset.__getitem__(1)\n",
    "print(i.size(), t.size())\n",
    "print(i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot_init(shape):\n",
    "        init_range = np.sqrt(6.0 / (shape[0] + shape[1]))\n",
    "        return torch.tensor(np.random.uniform(-init_range, init_range, size=shape), device=device, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.W_ih = nn.Parameter(glorot_init((input_size, hidden_size)))\n",
    "        self.W_hh = nn.Parameter(glorot_init((hidden_size, hidden_size)))\n",
    "        self.W_ho = nn.Parameter(glorot_init((hidden_size, output_size)))\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_size, device=device, dtype=torch.float32, requires_grad=True))\n",
    "        self.b_o = nn.Parameter(torch.zeros(output_size, device=device, dtype=torch.float32, requires_grad=True))\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # 입력과 은닉 상태의 가중치를 곱하고 편향을 더한 후 활성화 함수를 적용합니다.\n",
    "        hidden = torch.tanh(torch.matmul(input, self.W_ih) + torch.matmul(hidden, self.W_hh) + self.b_h)\n",
    "        # 은닉 상태와 출력 가중치를 곱하고 편향을 더한 후 활성화 함수를 적용합니다.\n",
    "        output = torch.matmul(hidden, self.W_ho) + self.b_o\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 3.759310588430851\n",
      "Epoch [20/100], Loss: 3.1497568982712765\n",
      "Epoch [30/100], Loss: 2.6561058531416224\n",
      "Epoch [40/100], Loss: 2.2034262799202127\n",
      "Epoch [50/100], Loss: 1.8921004924368352\n",
      "Epoch [60/100], Loss: 1.7474339261968086\n",
      "Epoch [70/100], Loss: 2.3039052111037233\n",
      "Epoch [80/100], Loss: 2.5629646463597076\n",
      "Epoch [90/100], Loss: 2.4684772897273937\n",
      "Epoch [100/100], Loss: 2.378567310089761\n"
     ]
    }
   ],
   "source": [
    "# 모델, 손실 함수, 옵티마이저 설정\n",
    "input_size = len(char_set)\n",
    "hidden_size = 128\n",
    "output_size = len(char_set)\n",
    "model = RNN(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 학습\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        hidden = model.init_hidden(inputs.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "\n",
    "        for i in range(inputs.size(1)):\n",
    "            output, hidden = model(inputs[:, i], hidden)\n",
    "            loss += criterion(output, targets[:, i])\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item() / inputs.size(1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
