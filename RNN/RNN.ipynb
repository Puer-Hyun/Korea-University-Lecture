{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Puer-Hyun/Lecture-Summary/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "QfMLIEt9PLx4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jqVhELIrJvaS"
      },
      "source": [
        "모델은 단어를 입력 받아 문장에서 다음 문자가 무엇인지 예측할 것입니다. 이 과정은 원하는 길이의 문장을 생성할 때까지 반복됩니다.\n",
        "\n",
        "간단하게 하기 위해, 큰 규모의 외부 데이터셋을 사용하지 않을 것입니다. 대신 몇 개의 문장을 정의하여 모델이 이 문장들로부터 어떻게 학습하는지 살펴볼 것입니다. 이 구현에서 진행될 과정은 다음과 같습니다:\n",
        "\n",
        "먼저, PyTorch 패키지와 함께 모델 구축에 사용할 nn 패키지를 가져옵니다. 또한, 데이터 전처리에 NumPy만 사용할 것입니다. 왜냐하면 Torch는 NumPy와 매우 잘 작동하기 때문입니다.\n",
        "\n",
        "첫째, 모델에 첫 단어 또는 첫 몇 개의 문자를 입력할 때 출력하려는 문장을 정의합니다.\n",
        "\n",
        "그런 다음 문장에 있는 모든 문자로 딕셔너리를 생성하고 정수에 매핑합니다. 이를 통해 입력 문자를 해당 정수(char2int)로 변환하고 반대로 정수를 문자(int2char)로 변환할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbju-ErAPOnO",
        "outputId": "aa1a81eb-4c41-4a86-caf1-d4d05fa7545b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'o', 'r', 'u', 'n', ' ', 'f', 'd', 'h', 'w', 'c', 'a', 'i', 'v', 'g', 'e', 'y', 'm'}\n",
            "{0: 'o', 1: 'r', 2: 'u', 3: 'n', 4: ' ', 5: 'f', 6: 'd', 7: 'h', 8: 'w', 9: 'c', 10: 'a', 11: 'i', 12: 'v', 13: 'g', 14: 'e', 15: 'y', 16: 'm'}\n",
            "{'o': 0, 'r': 1, 'u': 2, 'n': 3, ' ': 4, 'f': 5, 'd': 6, 'h': 7, 'w': 8, 'c': 9, 'a': 10, 'i': 11, 'v': 12, 'g': 13, 'e': 14, 'y': 15, 'm': 16}\n"
          ]
        }
      ],
      "source": [
        "text = ['hey how are you','good i am fine','have a nice day']\n",
        "\n",
        "# Join all the sentences together and extract the unique characters from the combined sentences\n",
        "chars = set(''.join(text))\n",
        "print(chars)\n",
        "\n",
        "# Creating a dictionary that maps integers to the characters\n",
        "int2char = dict(enumerate(chars))\n",
        "print(int2char)\n",
        "\n",
        "# Creating another dictionary that maps characters to integers\n",
        "char2int = {char: ind for ind, char in int2char.items()}\n",
        "print(char2int)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NNx9wjByJy1z"
      },
      "source": [
        "다음으로, 모든 문장이 표준 길이를 갖도록 입력 문장에 패딩을 적용합니다. RNN은 일반적으로 가변 길이의 입력을 처리할 수 있지만, 학습 과정을 가속화하기 위해 일반적으로 배치를 사용하여 학습 데이터를 입력합니다. 데이터를 배치로 학습하기 위해서는 입력 데이터 내의 각 시퀀스가 동일한 크기를 가져야 합니다.\n",
        "\n",
        "따라서 대부분의 경우, 길이가 너무 짧은 시퀀스는 0 값으로 채우고, 길이가 너무 긴 시퀀스는 자르는 방식으로 패딩을 수행할 수 있습니다. 우리의 경우, 가장 긴 시퀀스의 길이를 찾고 나머지 문장에 공백을 추가하여 해당 길이에 맞춥니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "TWHCc2n6HxNj"
      },
      "outputs": [],
      "source": [
        "# Finding the length of the longest string in our data\n",
        "maxlen = len(max(text, key=len))\n",
        "\n",
        "# Padding\n",
        "\n",
        "# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of\n",
        "# the sentence matches the length of the longest sentence\n",
        "for i in range(len(text)):\n",
        "  while len(text[i])<maxlen:\n",
        "      text[i] += ' '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLZ7OqXoIgXj",
        "outputId": "c852eca9-91da-4fc8-f481-6a4f56cee272"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[15, 15, 15]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[len(t) for t in text]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OqaSS6VNJ0mD"
      },
      "source": [
        "각 시간 단계에서 시퀀스의 다음 문자를 예측할 것이므로, 각 문장을 다음과 같이 나누어야 합니다:\n",
        "\n",
        "1. 입력 데이터:\n",
        "   모델에 입력할 필요가 없는 마지막 입력 문자는 제외합니다.\n",
        "2. 타겟/실제 레이블:\n",
        "   입력 데이터보다 한 시간 단계 앞서 있는데, 이것이 각 시간 단계에서 입력 데이터에 대한 모델의 \"정답\"이 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "War3GIPyIhGL",
        "outputId": "20124d00-12c6-4b28-83e0-bcf988a08213"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Sequence: hey how are yo\n",
            "Target Sequence: ey how are you\n",
            "Input Sequence: good i am fine\n",
            "Target Sequence: ood i am fine \n",
            "Input Sequence: have a nice da\n",
            "Target Sequence: ave a nice day\n"
          ]
        }
      ],
      "source": [
        "# Creating lists that will hold our input and target sequences\n",
        "input_seq = []\n",
        "target_seq = []\n",
        "\n",
        "for i in range(len(text)):\n",
        "    # Remove last character for input sequence\n",
        "  input_seq.append(text[i][:-1])\n",
        "    \n",
        "    # Remove first character for target sequence\n",
        "  target_seq.append(text[i][1:])\n",
        "  print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3Ps9rN0KKKj",
        "outputId": "bdc58008-b3a4-4015-b5dd-71b5d5b4777d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hey how are yo', 'good i am fine', 'have a nice da']"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_seq"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X7AbZIhRKZXT"
      },
      "source": [
        "입력 시퀀스와 타겟 시퀀스는 다음과 같이 표시됩니다:\n",
        "\n",
        "입력 시퀀스: hey how are yo\n",
        "타겟 시퀀스: ey how are you\n",
        "\n",
        "타겟 시퀀스는 항상 입력 시퀀스보다 한 시간 단계 앞서 있습니다.\n",
        "\n",
        "이제 위에서 생성한 사전을 사용하여 입력 시퀀스와 타겟 시퀀스를 문자 시퀀스가 아닌 정수 시퀀스로 변환할 수 있습니다. 이렇게 하면 입력 시퀀스를 원-핫 인코딩할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "zE5aVQehJa57"
      },
      "outputs": [],
      "source": [
        "for i in range(len(text)):\n",
        "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
        "    target_seq[i] = [char2int[character] for character in target_seq[i]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyejLw3gKSYj",
        "outputId": "803f8a8e-e2c3-428a-c026-631e65256e14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[7, 14, 15, 4, 7, 0, 8, 4, 10, 1, 14, 4, 15, 0],\n",
              " [13, 0, 0, 6, 4, 11, 4, 10, 16, 4, 5, 11, 3, 14],\n",
              " [7, 10, 12, 14, 4, 10, 4, 3, 11, 9, 14, 4, 6, 10]]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_seq"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2-uyzRa9KUir"
      },
      "source": [
        "원-핫 벡터로 입력 시퀀스를 인코딩하기 전에, 3가지 주요 변수를 정의하겠습니다:\n",
        "\n",
        "* dict_size: 사전 크기 - 텍스트에 있는 고유한 문자의 수\n",
        "이 값은 각 문자에 할당된 인덱스를 가진 벡터이기 때문에 원-핫 벡터의 크기를 결정합니다.\n",
        "* seq_len: 모델에 입력하는 시퀀스의 길이\n",
        "모든 문장의 길이를 가장 긴 문장과 같게 표준화했으므로, 이 값은 최대 길이 - 1이 됩니다. 마지막 문자 입력을 제거했기 때문입니다.\n",
        "* batch_size: 정의한 문장의 수로, 모델에 한 번에 입력할 배치 크기입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkGZdfymKrk7",
        "outputId": "38dcc8de-9ad7-478c-d233-f6d4a218c3b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_size: 17\n",
            "seq_len: 14\n",
            "batch_size: 3\n"
          ]
        }
      ],
      "source": [
        "dict_size = len(char2int)\n",
        "seq_len = maxlen - 1\n",
        "batch_size = len(text)\n",
        "print(\"dict_size:\", dict_size)\n",
        "print(\"seq_len:\", seq_len)\n",
        "print(\"batch_size:\", batch_size)\n",
        "\n",
        "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
        "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
        "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
        "    \n",
        "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
        "    for i in range(batch_size):\n",
        "        for u in range(seq_len):\n",
        "            features[i, u, sequence[i][u]] = 1\n",
        "    return features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qTruQwXoKx8r"
      },
      "source": [
        "\n",
        "우리는 각 문자에 대해 0으로 채워진 배열을 생성하고 해당 문자 인덱스를 1로 바꾸는 도우미 함수를 정의했습니다.\n",
        "\n",
        "데이터 전처리를 모두 완료했으므로 이제 데이터를 NumPy 배열에서 PyTorch의 고유한 데이터 구조인 Torch Tensor로 옮길 수 있습니다.\n",
        "\n",
        "이제 이 프로젝트의 재미있는 부분에 도달했습니다! Torch 라이브러리를 사용하여 모델을 정의할 것이며, 여기서 완전 연결층, 합성곱층, 기본 RNN 층, LSTM 층 등 다양한 층을 추가하거나 제거할 수 있습니다. 이 게시물에서는 기본 nn.rnn을 사용하여 RNN이 어떻게 사용될 수 있는지 간단한 예제를 보여줄 것입니다.\n",
        "\n",
        "모델을 구축하기 전에 PyTorch의 내장 기능을 사용하여 실행 중인 장치(CPU 또는 GPU)를 확인해봅시다. 이 구현에서는 훈련이 매우 간단하기 때문에 GPU가 필요하지 않습니다. 그러나 대규모 데이터셋과 수백만 개의 훈련 가능한 매개변수가 있는 모델로 진행하게 되면, 훈련 속도를 높이기 위해 GPU를 사용하는 것이 매우 중요해집니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8KQJV8MLYSr",
        "outputId": "ad486e9a-8db7-4a1d-bc9d-46313bfa44b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[7, 14, 15, 4, 7, 0, 8, 4, 10, 1, 14, 4, 15, 0],\n",
              " [13, 0, 0, 6, 4, 11, 4, 10, 16, 4, 5, 11, 3, 14],\n",
              " [7, 10, 12, 14, 4, 10, 4, 3, 11, 9, 14, 4, 6, 10]]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "o9BhwQGELUij"
      },
      "outputs": [],
      "source": [
        "# 각각의 문장이 알파벳 별로 16,4,10 과 같이 표현이 되었는데, 이것을 다시 원핫인코딩을 해주느라 \n",
        "# hey에서 e라는 알파벳은 기존에는 4 였지만, 바뀐 벡터에선 [0,0,0,0, 1,0,0,0,,,,,, 0] = 17차원 짜리로 변경되었다.\n",
        "# Input shape --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n",
        "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BJExmY9Ls1z",
        "outputId": "ce58b657-3bac-4623-bdd0-75453d5a1e18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 14, 17)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_seq.shape \n",
        "# 14인 이유는 각 시퀀스가 14글자로 이루어져 있고, 17인 이유는 사전에 표현된 알파벳 개수가 전체 17개이기 때문"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-rPh_peQChT",
        "outputId": "635e1b9e-bc7b-43bf-b71a-f5e41c5a5fb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "GfEenukuMCeb"
      },
      "outputs": [],
      "source": [
        "input_seq = torch.from_numpy(input_seq)\n",
        "input_seq = input_seq.to(device)\n",
        "target_seq = torch.Tensor(target_seq).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30n1R4XINJhj",
        "outputId": "b1cce345-3ff9-4737-db3a-5877feaf091a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[14., 15.,  4.,  7.,  0.,  8.,  4., 10.,  1., 14.,  4., 15.,  0.,  2.],\n",
              "        [ 0.,  0.,  6.,  4., 11.,  4., 10., 16.,  4.,  5., 11.,  3., 14.,  4.],\n",
              "        [10., 12., 14.,  4., 10.,  4.,  3., 11.,  9., 14.,  4.,  6., 10., 15.]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWrBu7OHNLXb",
        "outputId": "27fab615-6e25-4541-a95b-d2e3890bdc30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_seq"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cuBKXMlMN1ib"
      },
      "source": [
        "우리만의 신경망 모델을 구축하기 시작하려면, 모든 신경망 모듈에 대한 PyTorch의 기본 클래스(nn.module)를 상속하는 클래스를 정의할 수 있습니다. 그렇게 하면 생성자 아래에 모델에 대한 변수와 레이어를 정의할 수 있습니다. 이 모델에서는 RNN 1개 층과 전결합층만 사용할 것입니다. 전결합층은 RNN 출력을 원하는 출력 형태로 변환하는 역할을 담당합니다.\n",
        "\n",
        "또한 클래스 메소드로 forward() 아래에 순방향 전달 함수를 정의해야 합니다. 순방향 함수는 순차적으로 실행되므로, RNN 출력을 완전 연결된 층에 전달하기 전에 먼저 입력과 0으로 초기화된 은닉 상태를 RNN 층에 전달해야 합니다. 생성자에서 정의한 레이어를 사용하고 있음을 주의하세요.\n",
        "\n",
        "마지막으로 정의해야 할 메소드는 이전에 은닉 상태를 초기화하기 위해 호출한 메소드인 init_hidden()입니다. 이 메소드는 은닉 상태 모양의 0으로 채워진 텐서를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "8AkkrhNVNWa8"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Defining some parameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        #Defining the layers\n",
        "        # RNN Layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Initializing hidden state for first input using method defined below\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        \n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out = out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
        "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
        "        return hidden.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "znA_idHCOh4c"
      },
      "source": [
        "위에서 모델을 정의한 후, 관련 매개변수를 사용하여 모델을 인스턴스화하고 하이퍼파라미터를 정의해야 합니다. 아래에서 정의하는 하이퍼파라미터는 다음과 같습니다:\n",
        "\n",
        "n_epochs: 에포크 수 --> 모델이 전체 학습 데이터셋을 몇 번 반복할지 결정\n",
        "lr: 학습률 --> 모델이 역전파가 수행될 때마다 셀의 가중치를 업데이트하는 속도\n",
        "하이퍼파라미터에 대한 보다 깊이 있는 가이드를 원하신다면, 이 포괄적인 글을 참고하십시오.\n",
        "\n",
        "다른 신경망과 마찬가지로, 최적화기와 손실 함수도 정의해야 합니다. 최종 출력이 기본적으로 분류 작업이므로 CrossEntropyLoss를 사용하고 일반적인 Adam 최적화기를 사용할 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fT-3PwTOnNT",
        "outputId": "e290638d-ca70-41fb-e393-0356d8882c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17\n"
          ]
        }
      ],
      "source": [
        "print(dict_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrZjnn4DPC2s",
        "outputId": "94a0abf9-75e5-43d5-8509-e404b805396c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "xk5owtPvNfUD"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model with hyperparameters\n",
        "model = Model(input_size = dict_size, output_size = dict_size, hidden_dim=12, n_layers=1)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "model.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 100\n",
        "lr=0.01\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyhRLAJIOzn7",
        "outputId": "2db82cb3-fcb4-4bbf-9317-fa170c7683bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10/100............. Loss: 2.4524\n",
            "Epoch: 20/100............. Loss: 2.1997\n",
            "Epoch: 30/100............. Loss: 1.8606\n",
            "Epoch: 40/100............. Loss: 1.4217\n",
            "Epoch: 50/100............. Loss: 0.9857\n",
            "Epoch: 60/100............. Loss: 0.6517\n",
            "Epoch: 70/100............. Loss: 0.4256\n",
            "Epoch: 80/100............. Loss: 0.2864\n",
            "Epoch: 90/100............. Loss: 0.2044\n",
            "Epoch: 100/100............. Loss: 0.1554\n"
          ]
        }
      ],
      "source": [
        "# Training Run\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "    output, hidden = model(input_seq)\n",
        "    loss = criterion(output, target_seq.view(-1).long())\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "    \n",
        "    if epoch%10 == 0:\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "        print(\"Loss: {:.4f}\".format(loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "rLNASY4IO12M"
      },
      "outputs": [],
      "source": [
        "# This function takes in the model and character as arguments and returns the next character prediction and hidden state\n",
        "def predict(model, character):\n",
        "    # One-hot encoding our input to fit into the model\n",
        "    character = np.array([[char2int[c] for c in character]])\n",
        "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
        "    character = torch.from_numpy(character)\n",
        "    character = character.to(device)\n",
        "    \n",
        "    out, hidden = model(character)\n",
        "\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "    # Taking the class with the highest probability score from the output\n",
        "    char_ind = torch.max(prob, dim=0)[1].item()\n",
        "\n",
        "    return int2char[char_ind], hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "ry-A_ZsRQNFr"
      },
      "outputs": [],
      "source": [
        "# This function takes the desired output length and input characters as arguments, returning the produced sentence\n",
        "def sample(model, out_len, start='hey'):\n",
        "    model.eval() # eval mode\n",
        "    start = start.lower()\n",
        "    # First off, run through the starting characters\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    # Now pass in the previous characters and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(model, chars)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mDs5xiPuQOor",
        "outputId": "eb86e092-c50d-44eb-9c1c-1a07437aa75f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'good i am fine '"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample(model, 15, 'good')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hive a nice day hive a nice da'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample(model, 30, 'hi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN23iInUm6oH+ruFJ0uVFxD",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "practice",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
