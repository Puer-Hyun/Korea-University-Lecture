{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 문자 -> 인덱스 변환 함수\n",
    "def char_to_index(char, char_to_idx):\n",
    "    return char_to_idx[char]\n",
    "\n",
    "# 문자열 -> 원-핫 인코딩 변환 함수\n",
    "def string_to_one_hot(string, char_to_idx):\n",
    "    tensor = torch.zeros(len(string), len(char_to_idx))\n",
    "    for idx, char in enumerate(string):\n",
    "        tensor[idx][char_to_index(char, char_to_idx)] = 1\n",
    "    return tensor.to(device)\n",
    "\n",
    "# 데이터 수집\n",
    "response = requests.get(\"https://loripsum.net/api?plaintext&paragraphs=1000\")\n",
    "data = response.text\n",
    "sentences = data.split(\"\\n\")\n",
    "\n",
    "# 문자 집합 생성\n",
    "char_set = sorted(list(set(data)))\n",
    "char_to_idx = {char: idx for idx, char in enumerate(char_set)}\n",
    "\n",
    "# 패딩 작업을 수행하는 함수 (수정된 부분)\n",
    "def pad_sequences(sequences, max_length, padding_value):\n",
    "    padded_sequences = []\n",
    "    for sequence in sequences:\n",
    "        sequence = np.array(sequence) # 리스트를 Numpy 배열로 변환\n",
    "        padding_needed = max_length - sequence.shape[0]\n",
    "        if padding_needed > 0:\n",
    "            if sequence.ndim == 1:\n",
    "                sequence = np.hstack((sequence, np.full(padding_needed, padding_value)))\n",
    "            else:\n",
    "                sequence = np.vstack((sequence, np.full((padding_needed, sequence.shape[1]), padding_value)))\n",
    "        else:\n",
    "            sequence = sequence[:max_length]\n",
    "        padded_sequences.append(sequence)\n",
    "    return padded_sequences\n",
    "\n",
    "# 데이터셋 생성 (수정된 부분)\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, sentences, char_to_idx, max_length):\n",
    "        self.sentences = sentences\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.sentences[index]\n",
    "        input_data = string_to_one_hot(sentence[:-1], self.char_to_idx).cpu().numpy() # 원-핫 인코딩된 데이터를 Numpy 배열로 변환\n",
    "        target_data = [char_to_index(char, self.char_to_idx) for char in sentence[1:]]\n",
    "\n",
    "        # 패딩 작업 수행\n",
    "        input_data = pad_sequences([input_data], self.max_length, [0] * len(self.char_to_idx))[0] # 패딩 값으로 0 벡터를 사용\n",
    "        target_data = pad_sequences([target_data], self.max_length, 0)[0] # -100은 패딩 값입니다. 원하는 값으로 변경하세요.\n",
    "\n",
    "        input_data = torch.tensor(input_data, dtype=torch.float).to(device)\n",
    "        target_data = torch.tensor(target_data, dtype=torch.long).to(device)\n",
    "\n",
    "        return input_data, target_data\n",
    "\n",
    "# 문장의 최대 길이 설정\n",
    "max_length = max([len(sentence) for sentence in sentences])\n",
    "\n",
    "# 데이터셋 및 DataLoader 생성\n",
    "train_dataset = ToyDataset(sentences, char_to_idx, max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True) # 배치 크기를 원하는 값으로 변경하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot_init(shape):\n",
    "        init_range = np.sqrt(6.0 / (shape[0] + shape[1]))\n",
    "        return torch.tensor(np.random.uniform(-init_range, init_range, size=shape), device=device, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.W_ih = nn.Parameter(glorot_init((input_size, hidden_size)))\n",
    "        self.W_hh = nn.Parameter(glorot_init((hidden_size, hidden_size)))\n",
    "        self.W_ho = nn.Parameter(glorot_init((hidden_size, output_size)))\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_size, device=device, dtype=torch.float32, requires_grad=True))\n",
    "        self.b_o = nn.Parameter(torch.zeros(output_size, device=device, dtype=torch.float32, requires_grad=True))\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # 입력과 은닉 상태의 가중치를 곱하고 편향을 더한 후 활성화 함수를 적용합니다.\n",
    "        hidden = torch.tanh(torch.matmul(input, self.W_ih) + torch.matmul(hidden, self.W_hh) + self.b_h)\n",
    "        # 은닉 상태와 출력 가중치를 곱하고 편향을 더한 후 활성화 함수를 적용합니다.\n",
    "        output = torch.matmul(hidden, self.W_ho) + self.b_o\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 3.7878290857832138\n",
      "Epoch [20/100], Loss: 3.266110432212186\n",
      "Epoch [30/100], Loss: 2.9512082550213194\n",
      "Epoch [40/100], Loss: 2.5967439004291966\n",
      "Epoch [50/100], Loss: 2.558258878478456\n",
      "Epoch [60/100], Loss: 2.416263052976324\n",
      "Epoch [70/100], Loss: 2.331434486156306\n",
      "Epoch [80/100], Loss: 2.2706783164553412\n",
      "Epoch [90/100], Loss: 1.6612804145744502\n",
      "Epoch [100/100], Loss: 2.1722979725370286\n"
     ]
    }
   ],
   "source": [
    "# 모델, 손실 함수, 옵티마이저 설정\n",
    "input_size = len(char_set)\n",
    "hidden_size = 128\n",
    "output_size = len(char_set)\n",
    "model = RNN(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 학습\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        hidden = model.init_hidden(inputs.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "\n",
    "        for i in range(inputs.size(1)):\n",
    "            output, hidden = model(inputs[:, i], hidden)\n",
    "            loss += criterion(output, targets[:, i])\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item() / inputs.size(1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Philoushae.l\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "P\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rh\n",
      "N\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a\n",
      "i\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "i\n",
      "\n",
      "\n",
      "nf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "bc\n",
      "\n",
      "\n",
      "MQd\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "n\n",
      "/\n",
      "t\n",
      "<\n",
      "\n",
      "\n",
      "\n",
      "Tp\n",
      "\n",
      "/Q\n",
      "\n",
      "\n",
      "x\n",
      "\n",
      "E\n",
      "\n",
      "\n",
      "\n",
      ",p\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict(input_str, model, char_to_idx, idx_to_char, num_chars_to_generate, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_sequence = string_to_one_hot(input_str, char_to_idx)\n",
    "    hidden = model.init_hidden(1)\n",
    "\n",
    "    for i in range(input_sequence.size(0)):\n",
    "        output, hidden = model(input_sequence[i].unsqueeze(0), hidden)\n",
    "\n",
    "    predicted_chars = []\n",
    "    for _ in range(num_chars_to_generate):\n",
    "        output_dist = output.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = idx_to_char[top_i.item()]\n",
    "        predicted_chars.append(predicted_char)\n",
    "\n",
    "        input_sequence = string_to_one_hot(predicted_char, char_to_idx)\n",
    "        output, hidden = model(input_sequence.unsqueeze(0), hidden)\n",
    "\n",
    "    return input_str + \"\".join(predicted_chars)\n",
    "\n",
    "# 예측할 문자 개수 설정\n",
    "num_chars_to_generate = 100\n",
    "\n",
    "# 시작 문자열 설정\n",
    "input_str = \"Philo\"\n",
    "\n",
    "# 인덱스를 문자로 변환하는 사전 생성\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "# 예측 결과 출력\n",
    "predicted_text = predict(input_str, model, char_to_idx, idx_to_char, num_chars_to_generate)\n",
    "print(predicted_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
